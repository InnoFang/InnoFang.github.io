<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Hadoop分布式搭建全流程记录"><meta name="keywords" content="Hadoop"><meta name="author" content="Inno Fang"><meta name="copyright" content="Inno Fang"><title>Hadoop分布式搭建全流程记录 | Inno' Blog</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Inno' Blog" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE"><span class="toc-number">2.</span> <span class="toc-text">虚拟机配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B0%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">2.1.</span> <span class="toc-text">新建虚拟机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">2.2.</span> <span class="toc-text">配置虚拟机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96IP%E5%8F%8A%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99"><span class="toc-number">2.3.</span> <span class="toc-text">获取IP及关闭防火墙</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81IP"><span class="toc-number">2.4.</span> <span class="toc-text">配置静态IP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D"><span class="toc-number">2.5.</span> <span class="toc-text">修改主机名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%B8%80%E8%88%AC%E7%94%A8%E6%88%B7"><span class="toc-number">2.6.</span> <span class="toc-text">创建一个一般用户</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E6%96%B0%E5%BB%BA%E7%94%A8%E6%88%B7%E8%B5%8B%E4%BA%88-root-%E6%9D%83%E9%99%90"><span class="toc-number">2.6.1.</span> <span class="toc-text">为新建用户赋予 root 权限</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E7%BB%9F%E4%B8%80%E7%AE%A1%E7%90%86%E5%8E%8B%E7%BC%A9%E5%8C%85%E5%92%8C%E8%A7%A3%E5%8E%8B%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-number">2.7.</span> <span class="toc-text">创建统一管理压缩包和解压文件的文件夹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">2.8.</span> <span class="toc-text">克隆虚拟机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-hosts"><span class="toc-number">2.9.</span> <span class="toc-text">配置 hosts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-Xshell-%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-number">2.10.</span> <span class="toc-text">使用 Xshell 远程访问虚拟机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-number">2.11.</span> <span class="toc-text">配置ssh免密登录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop-%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE"><span class="toc-number">3.</span> <span class="toc-text">Hadoop 完全分布式配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E8%A7%84%E5%88%92%E5%8F%8A%E8%B5%84%E6%BA%90%E5%87%86%E5%A4%87"><span class="toc-number">3.1.</span> <span class="toc-text">集群部署规划及资源准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AE-JDK"><span class="toc-number">3.2.</span> <span class="toc-text">安装并配置 JDK</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AE-Hadoop"><span class="toc-number">3.3.</span> <span class="toc-text">安装并配置 Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8B%E5%B9%B6%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">3.3.1.</span> <span class="toc-text">解压并配置环境变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E7%9A%84%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">3.3.2.</span> <span class="toc-text">Hadoop的目录结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4"><span class="toc-number">3.3.3.</span> <span class="toc-text">配置集群</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE"><span class="toc-number">3.3.3.1.</span> <span class="toc-text">核心文件配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS-%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE"><span class="toc-number">3.3.3.2.</span> <span class="toc-text">HDFS 文件配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN-%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE"><span class="toc-number">3.3.3.3.</span> <span class="toc-text">YARN 文件配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce-%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE"><span class="toc-number">3.3.3.4.</span> <span class="toc-text">MapReduce 文件配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-slaves"><span class="toc-number">3.3.3.5.</span> <span class="toc-text">配置 slaves</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%96%87%E4%BB%B6%E5%8F%8A%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83"><span class="toc-number">3.3.3.6.</span> <span class="toc-text">同步虚拟机文件及配置环境</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-number">4.</span> <span class="toc-text">启动集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%BC%E5%BC%8F%E5%8C%96-NameNode"><span class="toc-number">4.1.</span> <span class="toc-text">格式化 NameNode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8HDFS%E5%92%8CYARN"><span class="toc-number">4.2.</span> <span class="toc-text">启动HDFS和YARN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">4.3.</span> <span class="toc-text">启动历史服务器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B"><span class="toc-number">4.4.</span> <span class="toc-text">运行官方示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%9C%E6%AD%A2%E9%9B%86%E7%BE%A4"><span class="toc-number">4.5.</span> <span class="toc-text">停止集群</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">5.</span> <span class="toc-text">其他</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="toc-number">5.1.</span> <span class="toc-text">集群时间同步</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5NTP%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8"><span class="toc-number">5.1.1.</span> <span class="toc-text">检查NTP服务是否存在</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5NTPD%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E5%BC%80%E5%90%AF"><span class="toc-number">5.1.2.</span> <span class="toc-text">检查NTPD服务是否开启</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9NTP%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">5.1.3.</span> <span class="toc-text">修改NTP配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9-etc-sysconfig-ntpd%E6%96%87%E4%BB%B6"><span class="toc-number">5.1.4.</span> <span class="toc-text">修改&#x2F;etc&#x2F;sysconfig&#x2F;ntpd文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E5%90%AFNTP%E6%9C%8D%E5%8A%A1%E5%B9%B6%E8%AE%BE%E7%BD%AE%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF"><span class="toc-number">5.1.5.</span> <span class="toc-text">重启NTP服务并设置开机自启</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%85%B6%E5%AE%83%E6%9C%BA%E5%99%A8%E4%B8%8E%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%90%8C%E6%AD%A5"><span class="toc-number">5.1.6.</span> <span class="toc-text">配置其它机器与时间服务器进行同步</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://avatars0.githubusercontent.com/u/15724026?s=400&amp;u=ee5d1bee0d44d91b01465f82910464fda54b91cc&amp;v=4"></div><div class="author-info__name text-center">Inno Fang</div><div class="author-info__description text-center">Programming is an art form</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/InnoFang">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">64</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">17</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">17</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链 / links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://wmpscc.github.io/">HeoLis</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="http://barackbao.com/">Brarack Bao</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://wanghao15536870732.github.io/">HyYyr Wang</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://www.ingernotes.cn/">Inger Chao</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://www.angus-liu.cn/">Angus Liu 's Blog</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://ww1.sinaimg.cn/large/0067fiZ7ly1g1i7czlxxhj318g0rsti8.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Inno' Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/gallery">Gallery</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">Hadoop分布式搭建全流程记录</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/">Hadoop</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本次Hadoop分布式环境搭建，最终目标是使用三个虚拟机来形成一个小的分布式集群，并可以在本机中通过主机名加端口的方式访问到虚拟机中的 HDFS 和 YARN，也就是说：可以在本机（Windows/Mac OS/Linux）中开发，再通过远程连接虚拟机来运行 MapReduce 程序。</p>
<span id="more"></span>

<p>本次搭建过程中，所用资源及其版本如下如下：</p>
<ul>
<li><strong>虚拟机：</strong> VMware Workstation 15.5 pro</li>
<li><strong>CentOS:</strong>  <a target="_blank" rel="noopener" href="http://mirrors.aliyun.com/centos/7/isos/x86_64/">CentOS-7-x86_64-DVD-2003</a></li>
<li><strong>JDK:</strong>  <a target="_blank" rel="noopener" href="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html">jdk-8u251-linux-x64</a></li>
<li><strong>Hadoop：</strong> <a target="_blank" rel="noopener" href="http://archive.cloudera.com/cdh5/cdh/5/">hadoop-2.6.0-cdh5.15.0</a></li>
<li><strong>远程连接工具：</strong> <a target="_blank" rel="noopener" href="https://www.netsarang.com/zh/free-for-home-school/">Xshell 和 Xftp</a> （个人可以通过邮件获取免费正版）</li>
</ul>
<p><strong>注意</strong> 本文不涉及软件破解过程，如果介意该过程可以使用 <a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a> 作为替代，虽然在使用流程上会与 VMware 稍有不同，但是一样可以达到目的。此外，本文中涉及虚拟机的问题也将与之不同，如在 VirtualBox 的安装配置过程中遇到问题还请查阅其他文章。</p>
<h2 id="虚拟机配置"><a href="#虚拟机配置" class="headerlink" title="虚拟机配置"></a>虚拟机配置</h2><h3 id="新建虚拟机"><a href="#新建虚拟机" class="headerlink" title="新建虚拟机"></a>新建虚拟机</h3><p>首先打开 VMWare Workstation，逐步进行以下流程：</p>
<ol>
<li>点击菜单栏文件 </li>
<li>新建虚拟机</li>
<li>选择 “典型(推荐)”</li>
<li>安装程序光盘映像文件，选择下载好的 CentOS 镜像文件</li>
<li>修改虚拟机名字（这一步是为了方便管理虚拟机集群，修改名称诸如 <code>hadoop101</code> 之类易于分辨的皆可）</li>
<li>往后一直默认确认，直到虚拟机创建完毕</li>
</ol>
<p>接着需要修改虚拟机配置，基本配置要求如下：</p>
<ul>
<li><strong>硬盘：</strong> 20G（视自己硬盘空间而定，默认最低为 20G，后续不够还可以手动扩容）</li>
<li><strong>内存：</strong> 2G（一般 2G 就够了）</li>
<li><strong>处理器：</strong> 2 x 2 （即处理器数量和每个处理器的内核数量为 2 x 2，处理器内核总数为 4，如果还需要更高，需要查看自己CPU的个数与核数，处理器总核数不能超过本机上限）</li>
<li><strong>网络适配器：</strong> NAT 模式（VMware 默认配置）</li>
</ul>
<h3 id="配置虚拟机"><a href="#配置虚拟机" class="headerlink" title="配置虚拟机"></a>配置虚拟机</h3><p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%AE%BE%E7%BD%AE.jpg" alt="虚拟机配置"></p>
<p>确认后即可点击 “开启此虚拟机” 进行虚拟机的启动项配置，语言设置选择中文</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E8%AF%AD%E8%A8%80%E9%80%89%E6%8B%A9.jpg" alt="语言选择"></p>
<p>确认后进入 “安装信息摘要”，先配置 “安装位置”</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%AE%89%E8%A3%85%E4%BF%A1%E6%81%AF%E6%91%98%E8%A6%81.jpg" alt="安装信息摘要"></p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%AE%89%E8%A3%85%E7%9B%AE%E6%A0%87%E4%BD%8D%E7%BD%AE.jpg" alt="安装目标位置"></p>
<p>这里为了方便，我选择了 “自动配置分区”。点击 “完成”，回到 “安装信息摘要界面” 后，再点击 “网络和主机名”，打开以太网，点击 “完成”</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E7%BD%91%E7%BB%9C%E5%92%8C%E4%B8%BB%E6%9C%BA%E5%90%8D.jpg" alt="网络和主机名"></p>
<p>继续下一步</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E7%94%A8%E6%88%B7%E9%85%8D%E7%BD%AE.jpg" alt="用户配置"></p>
<p>这里要为root配置密码，等会登陆后需要用到，暂时可以不用创建用户。稍等片刻，安装完成后，选择 “重启”。</p>
<p>重启后会看到终端交互界面，输入root和密码即可登录成功</p>
<p><strong>值得注意的是</strong>，在分布式集群的使用过程中，是不需要图形界面的，全程会使用 Xshell 这类远程终端工具来远程链接操作集群的系统，文件传输可以使用 Xftp 来完成，也可以使用其他类似的工具，比如 SecureCRT。当然，如果不习惯无操作界面的系统或者其他原因需要 GUI，那么可以在 CentOS 启动项配置过程中的 <code>安装信息摘要 -&gt; 软件 -&gt; 软件选择</code> 中选择自己需要的组件，不过使用默认的 “最小安装” 才是符合实际开发场景的。（GUI 安装选项不是本文重点并且也不需要，可以自行查阅其他文章）</p>
<h3 id="获取IP及关闭防火墙"><a href="#获取IP及关闭防火墙" class="headerlink" title="获取IP及关闭防火墙"></a>获取IP及关闭防火墙</h3><p>紧接着输入 <code>ip addr</code> 获取本机的 IP 地址，当然<strong>前提</strong>是在安装时已经打开了以太网的连接</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E8%8E%B7%E5%8F%96IP%E5%9C%B0%E5%9D%80.jpg" alt="获取IP地址"></p>
<p>下一步，我们需要将虚拟机的防火墙关闭，这样主机才能 ping 通虚拟机的 IP。相关操作如下所示：</p>
<ul>
<li>启动防火墙：<code>systemctl start firewalld.service</code></li>
<li>关闭防火墙：<code>systemctl stop firewalld.service</code></li>
<li>重启防火墙：<code>systemctl restart firewalld.service</code></li>
<li>显示防火墙状态：<code>systemctl status firewalld.service</code></li>
<li>禁止防火墙开机自启：<code>systemctl disable firewalld.service</code></li>
</ul>
<p>要注意的是，CentOS 7 的防火墙操作指令与 CentOS 6 的不同，如果你是 CentOS 6 的操作系统还请自行查阅相关操作。<br>这里我们需要<strong>先关闭防火墙</strong>，<strong>再禁止防火墙开机自启</strong>，<strong>最后查看一下防火墙状态</strong>，出现如下信息则表示防火墙已关闭。</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99.jpg" alt="关闭防火墙"></p>
<p>接着主机可以尝试 ping 一下虚拟机的 IP，如果能 ping 通则说明没有问题。</p>
<h3 id="配置静态IP"><a href="#配置静态IP" class="headerlink" title="配置静态IP"></a>配置静态IP</h3><p>首先在控制台输入 <code>vi /etc/sysconfig/network-scripts/ifcfg-eth0</code></p>
<p>输入如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=eth0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">IPADDR=192.168.15.133</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.15.2</span><br><span class="line">DNS1=192.168.15.2</span><br><span class="line">NAME=eth0</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9%E9%9D%99%E6%80%81IP.jpg" alt="修改静态IP"></p>
<p><strong>注意</strong> <code>IPADDR</code> 是该台虚拟机的 IP 地址，所以要与你上一步获取到的 IP 地址相一致。此外，对于 <code>GATEWAY</code> 的值，需要查看当前虚拟机的网关，并与之保持一致，<code>DNS1</code> 的值与 <code>GATEWAY</code> 的值相同即可。获取步骤如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%89%93%E5%BC%80%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E7%BC%96%E8%BE%91%E5%99%A8.jpg" alt="虚拟机打开虚拟网络编辑器"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E7%BC%96%E8%BE%91%E5%99%A8.jpg" alt="虚拟网络编辑器"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%9F%A5%E7%9C%8B%E7%BD%91%E5%85%B3IP.jpg" alt="查看网关IP"></p>
<h3 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h3><p>接下来修改主机名，这样做是为了在打开多个系统的终端时，可以分得清对应的是哪个。输入 <code>vi /etc/sysconfig/network</code>，输入如下内容（主机名为 <code>hadoop101</code>），保存之后，重启即可看到主机名已经修改。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=hadoop101</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D.jpg" alt="修改主机名"></p>
<h3 id="创建一个一般用户"><a href="#创建一个一般用户" class="headerlink" title="创建一个一般用户"></a>创建一个一般用户</h3><p>如果是在自己安装的虚拟机上操作，为了省事可以省略这一步，但是实际操作过程中，是<strong>不建议直接使用 root 用户进行操作</strong>的。主要原因在于 root 的权限过大，如果在 root 权限下进行了误操作，那么造成的损失会非常大，此外从安全的角度考虑，以 root 身份运行的程序被攻击时，就会直接获取到 root 的密码，其后果也是不堪设想的。对于开发人员来说，普通用户的权限也能够满足绝大部分需求，而且对于有多种用途的 linux 来说，只有一个 root 账号是不便于管理的。因此，<strong>本文还是建议按照步骤执行，不要省略这一步</strong>。</p>
<p>创建新用户的方式很简单，比如创建一个名为 <code>inno</code> 的用户（用户名请自行替换）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建新用户，用户名为 inno，请自行替换</span></span><br><span class="line">adduser inno</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建新用户的密码，</span></span><br><span class="line">passwd inno</span><br></pre></td></tr></table></figure>

<p>可以通过 <code>ll /home</code> 看到刚才创建的用户目录，此时的用户权限较低，为了之后的操作方便，先为新创建的用户赋予 root 权限</p>
<h4 id="为新建用户赋予-root-权限"><a href="#为新建用户赋予-root-权限" class="headerlink" title="为新建用户赋予 root 权限"></a>为新建用户赋予 root 权限</h4><p>步骤如下：</p>
<ol>
<li>使用命令 <code>chmod -v u+w /etc/sudoers</code> 设置 sudoers 文件权限为可读写</li>
<li>使用命令 <code>vi /etc/sudoers</code> 编辑 sudoers 文件</li>
<li>找到 <code>root    ALL=(ALL)       ALL</code> 这一行并在下一行添加新建的用户名，如下：<ul>
<li>添加该行后就可以使用 sudo 行使 root 权限 <code>inno    ALL=(ALL)       ALL</code></li>
<li><strong>若想在使用 sudo 时不输入密码</strong>，则可更改为  <code>inno    ALL=(ALL)       NOPASSWD: ALL</code></li>
<li>保存退出</li>
</ul>
</li>
<li>使用命令 <code>chmod -v u-w /etc/sudoers</code>删除 sudoers 写权限</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E4%B8%BA%E6%96%B0%E5%BB%BA%E7%94%A8%E6%88%B7%E6%B7%BB%E5%8A%A0root%E6%9D%83%E9%99%90.jpg" alt="为新建用户添加root权限"></p>
<p>使用命令 <code>su inno</code>，切换到 <code>inno</code> 用户下，测试一下 <code>sudo ls</code>，如果没有报错，则说明配置成功。</p>
<h3 id="创建统一管理压缩包和解压文件的文件夹"><a href="#创建统一管理压缩包和解压文件的文件夹" class="headerlink" title="创建统一管理压缩包和解压文件的文件夹"></a>创建统一管理压缩包和解压文件的文件夹</h3><p>在 <code>/opt</code> 下创建 <code>module</code>、<code>software</code> 文件夹，并修改两个文件夹的所有者</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /opt/module</span><br><span class="line">sudo mkdir /opt/software</span><br><span class="line">sudo chown inno:inno /opt/module /opt/software</span><br></pre></td></tr></table></figure>

<p>之后压缩包会统一放在 <code>/opt/software</code> 目录下，解压后的文件会放在 <code>/opt/module</code> 目录下</p>
<h3 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h3><p>因为最终的目的是使用多台虚拟机形成一个小集群，所以上面的基本操作是适用于往后所有用作集群的虚拟机的，因此，我们需要在上述操作都完成后，为虚拟机新建一个节点，这样在之后克隆虚拟机时就可以从当前节点克隆，操作流程如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86%E5%99%A8.jpg" alt="快照管理器"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%8B%8D%E6%91%84%E5%BF%AB%E7%85%A7.jpg" alt="拍摄快照"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%96%B0%E7%9A%84%E5%BF%AB%E7%85%A7%E8%8A%82%E7%82%B9.jpg" alt="新的快照节点"></p>
<p><strong>注意</strong>，克隆操作需要关闭当前运行的虚拟机后才能运行</p>
<p><strong>Q：</strong> 为什么我们要从一个干净的 Linux 虚拟机开始克隆？可否在配置完一台虚拟机的 Hadoop 环境后再进行克隆？</p>
<p><strong>A：</strong> 按理说是可以的，特别是对于在自己本机上用虚拟机搭建的分布式环境来说，这样在配置好一台 Hadoop 环境后再克隆可以方便很多。但是在实际开发过程中，往往用的不会是虚拟机，而是在服务器上，这就没有办法进行克隆操作了，因此学习如何远程同步多台虚拟机的配置环境是很有必要的。</p>
<p>选择 “clean” 节点，点击右下的 “克隆” 按钮，再克隆两台虚拟机，并且虚拟机名字分别为 <code>hadoop102</code> 和 <code>hadoop103</code>。</p>
<p><strong>注意</strong> 对于新克隆的虚拟机，还需要执行两个步骤，参照 <a href="#%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81IP"><code>配置静态IP</code></a> 和 <a href="#%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D"><code>修改主机名</code></a> 这两步的操作，分别设置对应的静态 IP 地址和网关，以及修改新克隆的两台虚拟机的主机名为 <code>hadoop102</code> 和 <code>hadoop103</code>，然后重启即可。</p>
<h3 id="配置-hosts"><a href="#配置-hosts" class="headerlink" title="配置 hosts"></a>配置 hosts</h3><p>将三台虚拟机都打开并登陆后，输入 <code>ip addr</code> 查看各自的 IP 地址，本文中三台虚拟机的 IP 地址和主机名的映射关系如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.15.133 hadoop101</span><br><span class="line">192.168.15.134 hadoop102</span><br><span class="line">192.168.15.136 hadoop103</span><br></pre></td></tr></table></figure>

<p>接着需要把这些内容复制到每台虚拟机的 host 文件中，在每台虚拟机终端中输入命令 <code>vi /etc/hosts</code> ，将内容复制进去即可。</p>
<p>如果不想挨个操作，可以在修改完 <code>hadoop101</code> 的 <code>/etc/hosts</code> 文件后，就在 <code>hadoop101</code> 这台虚拟机的终端上使用命令拷贝到另外两台虚拟机的对应位置上（IP 地址为另外两台虚拟机的IP地址）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/hosts root@192.168.15.134:/etc/</span><br><span class="line">scp /etc/hosts root@192.168.15.136:/etc/</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>，还需要在你 Windows 的 hosts 文件中配置以上 IP 映射，否则后面无法通过主机名链接到 HDFS。Windows 的 hosts 文件路径为 <code>C:\Windows\System32\drivers\etc\HOSTS</code>，以管理员身份打开，并将上面的 IP 地址映射添加到文件末尾后保存（Mac OS 的添加方式自行查阅）。</p>
<h3 id="使用-Xshell-远程访问虚拟机"><a href="#使用-Xshell-远程访问虚拟机" class="headerlink" title="使用 Xshell 远程访问虚拟机"></a>使用 Xshell 远程访问虚拟机</h3><p>实际开发环境中，是没法直接去操作节点的，这时候就需要使用 Xshell 这类远程终端工具来访问。在自己的电脑上操作时也很方便，只要第一次连接成功后，之后每次只需打开虚拟机就可以放到后台了，直接操作终端界面会比在虚拟机之间来回切换方便很多。同样的还有一款 Xftp 文件传输工具，我们也可以用类似的方式通过远程连接来往节点传送文件。对于 Xshell 连接步骤如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/Xshell%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A5.jpg" alt="Xshell新建连接"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/Xshell%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E8%BE%93%E5%85%A5IP.jpg" alt="Xshell远程连接输入IP"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/Xshell%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E8%BE%93%E5%85%A5%E7%94%A8%E6%88%B7%E5%90%8D.jpg" alt="Xshell远程连接输入用户名"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/Xshell%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E8%BE%93%E5%85%A5%E5%AF%86%E9%92%A5.jpg" alt="Xshell远程连接输入密钥"></p>
<p>如果是第一次连接，则会提示是否保存主机密钥，点击保存即可。</p>
<p>Xftp 的连接方式类似，新建连接，然后输入 IP 地址，之后选择 IP 地址后再输入用户名和密码即可。</p>
<p>之后的所有操作都将通过 Xshell 来完成，虚拟机只会放到后台运行。</p>
<h3 id="配置ssh免密登录"><a href="#配置ssh免密登录" class="headerlink" title="配置ssh免密登录"></a>配置ssh免密登录</h3><p>集群之间的主机需要能够相互进行通信，因此配置 ssh 免密登录十分重要。先在 <code>hadoop101</code> 上输入以下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>然后可以一直敲回车直到结束。接着拷贝生成的公钥到另外两台虚拟机上，在 <code>hadoop101</code> 上输入以下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop102</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop103</span><br></pre></td></tr></table></figure>

<p>这样就将 <code>hadoop101</code> 的 ssh 密钥拷贝到了 <code>hadoop102</code> 和 <code>hadoop103</code> 上。<strong>另外两台虚拟机也需要执行上面两步</strong>，要注意修改要拷贝到的虚拟机的主机名（对于 <code>haoop102</code> 的密钥，就是要拷贝到 <code>hadoop101</code> 和 <code>hadoop103</code> 上）。</p>
<p>然后测试一下是否可以连接成功，比如在 <code>hadoop101</code> 上输入 <code>ssh hadoop102</code>，如果出现如下类似结果即表示成功</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E6%B5%8B%E8%AF%95.jpg" alt="ssh免密登录测试"></p>
<h2 id="Hadoop-完全分布式配置"><a href="#Hadoop-完全分布式配置" class="headerlink" title="Hadoop 完全分布式配置"></a>Hadoop 完全分布式配置</h2><h3 id="集群部署规划及资源准备"><a href="#集群部署规划及资源准备" class="headerlink" title="集群部署规划及资源准备"></a>集群部署规划及资源准备</h3><table>
<thead>
<tr>
<th>&nbsp;</th>
<th>hadoop101</th>
<th>hadoop102</th>
<th>hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td><strong>HDFS</strong></td>
<td>DataNode <br/> NameNode</td>
<td>DataNode</td>
<td>DataNode <br/> SecondaryNameNode</td>
</tr>
<tr>
<td><strong>YARN</strong></td>
<td>NodeManager</td>
<td>NodeManager <br/> ResourceManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<p>因为资源有限，所以在该集群中只是用了三台虚拟机，<code>hadoop101</code> 既是 HDFS 集群的主机（包含NameNode）也是 HDFS 集群的从机（连同另外两台都包含 DataNode）；<code>hadoop102</code> 既是 YARN 集群的主机（包含 ResourceManager）也是 YARN 集群的从机（连同另外两台）。</p>
<p>接下来开始按照集群部署规划开始安装配置，在此之前，还需要先将下载好的 JDK 和 Hadoop 压缩包通过 Xftp 传入到 <code>hadoop101</code> 上。打开 Xftp 然后确保已成功连接上 <code>hadoop101</code>，在左边的 Windows 文件树中找到 JDK 和 Hadoop 压缩包，然后再在右边找到虚拟机的 <code>/opt/software</code> 目录，直接将左边的文件拖到到右边的文件夹即可</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E4%BD%BF%E7%94%A8Xftp%E4%BC%A0%E9%80%81%E8%B5%84%E6%BA%90.jpg" alt="使用Xftp传送资源"></p>
<p><strong>往后的操作会先在 <code>hadoop101</code> 上进行资源解压和配置，然后再利用同步的方式，将解压文件和配置环境同步到另外两台虚拟机上。</strong></p>
<h3 id="安装并配置-JDK"><a href="#安装并配置-JDK" class="headerlink" title="安装并配置 JDK"></a>安装并配置 JDK</h3><blockquote>
<p>如果 CentOS 7 安装的是带 GUI 的版本，那么系统会自带 Java，因此还需要先删除系统自带的 JDK 再安装自己的JDK，删除方法自行查阅。</p>
</blockquote>
<p>先解压 JDK 到 <code>/opt/module</code> 目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[inno@hadoop101 ~]$ cd /opt/software/</span><br><span class="line">[inno@hadoop101 software]$ tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module</span><br></pre></td></tr></table></figure>

<p>再来配置 JDK 环境变量，这里先获取 JDK 的路径，以便后面的操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[inno@hadoop101 jdk1.8.0_251]$ pwd</span><br><span class="line">/opt/module/jdk1.8.0_251</span><br></pre></td></tr></table></figure>

<p>接着输入 <code>sudo vi /etc/profile</code> 打开 <code>/etc/profile</code> 文件，并在末尾添加 JDK 的环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>保存退出后输入 <code>source /etc/profile</code> 让配置文件生效，测试一下，若出现如下信息则表示 JDK 配置成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[inno@hadoop101 jdk1.8.0_251]$ java -version</span><br><span class="line">java version &quot;1.8.0_251&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_251-b08)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.251-b08, mixed mode)</span><br></pre></td></tr></table></figure>

<p>接着另外两台虚拟机也可以按照上述步骤手动安装，也可以使用以下命令同步到另外两台虚拟机</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo rsync -av /opt/module/jdk1.8.0_251 hadoop102:/opt/module/</span><br><span class="line">sudo rsync -av /opt/module/jdk1.8.0_251 hadoop103:/opt/module/</span><br><span class="line">sudo rsync -av /etc/profile hadoop102:/etc/profile</span><br><span class="line">sudo rsync -av /etc/profile hadoop103:/etc/profile</span><br></pre></td></tr></table></figure>

<p>rsync 是一种远程同步工具，具有速度快、避免复制相同内容和支持符号链接的优点。相比于上面使用过的 scp，用 rsync 做文件的复制要比 scp 的速度快，并且 rsync 只对差异文件做更新而 scp 是把所有文件都复制过去。这在之后会有很多的用途。<strong>在使用之前还需要在每台虚拟机上运行如下代码快速安装</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install rsync -y</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong> 将 JDK 和 <code>/etc/profile</code> 文件同步完成后，还需要到另外两台虚拟机上执行 <code>source /etc/profile</code> 命令，使配置生效，然后再测试一下 <code>java -version</code>，确保输出正确信息，确认无误后方可进行接下来的操作。</p>
<h3 id="安装并配置-Hadoop"><a href="#安装并配置-Hadoop" class="headerlink" title="安装并配置 Hadoop"></a>安装并配置 Hadoop</h3><h4 id="解压并配置环境变量"><a href="#解压并配置环境变量" class="headerlink" title="解压并配置环境变量"></a>解压并配置环境变量</h4><p>同样的，先到 <code>hadoop101</code> 上的 <code>/opt/software</code> 找到 Hadoop 的压缩包，解压到 <code>/opt/module</code> 目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[inno@hadoop101 ~]$ cd /opt/software/</span><br><span class="line">[inno@hadoop101 software]$ tar -zxvf hadoop-2.6.0-cdh5.15.0.tar.gz.tar -C /opt/module</span><br></pre></td></tr></table></figure>

<p>接着获取 hadoop 解压后的文件路径，以便等下配置 Hadoop 的环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[inno@hadoop101 hadoop-2.6.0-cdh5.15.0]$ pwd</span><br><span class="line">/opt/module/hadoop-2.6.0-cdh5.15.0</span><br></pre></td></tr></table></figure>

<p>接着输入 <code>sudo vi /etc/profile</code> 打开 <code>/etc/profile</code> 文件，并在末尾添加 Hadoop 的环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.6.0-cdh5.15.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>保存退出后输入 <code>source /etc/profile</code> 让配置文件生效，测试一下，若出现如下信息则表示 Hadoop 配置成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[inno@hadoop101 hadoop-2.6.0-cdh5.15.0]$ hadoop version</span><br><span class="line">Hadoop 2.6.0-cdh5.15.0</span><br><span class="line">Subversion http://github.com/cloudera/hadoop -r e3cb23a1cb2b89d074171b44e71f207c3d6ffa50</span><br><span class="line">Compiled by jenkins on 2018-05-24T11:22Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum 2efeda2e2d6b27ecf59ab56fffd9881</span><br><span class="line">This command was run using /opt/module/hadoop-2.6.0-cdh5.15.0/share/hadoop/common/hadoop-common-2.6.0-cdh5.15.0.jar</span><br></pre></td></tr></table></figure>

<h4 id="Hadoop的目录结构"><a href="#Hadoop的目录结构" class="headerlink" title="Hadoop的目录结构"></a>Hadoop的目录结构</h4><p>这里临时阐述一下 Hadoop 的目录结构，是为了之后配置一些文件的时候，能大致明白其作用</p>
<ol>
<li><code>bin</code> 目录：存放对 Hadoop 相关服务（HDFS,YARN）进行操作的脚本</li>
<li><code>etc</code> 目录：Hadoop 的配置文件目录，存放 Hadoop 的配置文件</li>
<li><code>lib</code> 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能）</li>
<li><code>sbin</code> 目录：存放启动或停止 Hadoop 相关服务的脚本</li>
<li><code>share</code> 目录：存放Hadoop的依赖 jar 包、文档、和官方案例</li>
</ol>
<p>只后要配置的所有文件的位置，都在你 hadoop 解压目录下的 <code>etc/hadoop</code> 目录下</p>
<h4 id="配置集群"><a href="#配置集群" class="headerlink" title="配置集群"></a>配置集群</h4><h5 id="核心文件配置"><a href="#核心文件配置" class="headerlink" title="核心文件配置"></a>核心文件配置</h5><p>配置 <code>core-site.xml</code>，输入 <code>vi core-site.xml</code>，并在文件中添加如下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop101:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.6.0-cdh5.15.0/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


<h5 id="HDFS-文件配置"><a href="#HDFS-文件配置" class="headerlink" title="HDFS 文件配置"></a>HDFS 文件配置</h5><p>配置 <code>hadoop-env.sh</code>，输入 <code>vi hadoop-env.sh</code>，，找到 <code># The java implementation to use.</code> 这一行</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E9%85%8D%E7%BD%AEhadoop-env.jpg" alt="配置hadoop-env"></p>
<p>将下面一行的 <code>JAVA_HOME</code> 修改为自己的</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br></pre></td></tr></table></figure>

<p>配置 <code>hdfs-site.xml</code>，输入 <code>vi hdfs-site.xml</code>，并在文件中添加如下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HFDS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="YARN-文件配置"><a href="#YARN-文件配置" class="headerlink" title="YARN 文件配置"></a>YARN 文件配置</h5><p>配置 <code>yarn-env.sh</code>，输入 <code>vi yarn-env.sh</code>，找到 <code># some Java parameters</code> 这一行</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E9%85%8D%E7%BD%AEyarn-env.jpg" alt="配置yarn-env"></p>
<p>取消在这一行下面的 <code>JAVA_HOME</code> 注释，并修改为自己的 <code>JAVA_HOME</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br></pre></td></tr></table></figure>

<p>配置 <code>yarn-site.xml</code>，输入 <code>vi yarn-site.xml</code>，并在文件中添加如下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="MapReduce-文件配置"><a href="#MapReduce-文件配置" class="headerlink" title="MapReduce 文件配置"></a>MapReduce 文件配置</h5><p>配置 <code>mapred-env.sh</code>，输入 <code>vi mapred-env.sh</code>，找到被注释的 <code># export JAVA_HOME=...</code>，</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E9%85%8D%E7%BD%AEmapred-env.jpg" alt="配置mapred-env"></p>
<p>取消该注释，并修改为自己的 <code>JAVA_HOME</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br></pre></td></tr></table></figure>

<p>配置 <code>mapred-site.xml</code>，因为一开始是没有该文件的，需要先拷贝 <code>mapred-site.xml.template</code> 并重命名为 <code>mapred-site.xml</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>输入 <code>vi mapred-site.xml</code>，并在文件中添加如下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="配置-slaves"><a href="#配置-slaves" class="headerlink" title="配置 slaves"></a>配置 slaves</h5><p>配置从节点的主机名，如果有按照之前的步骤在 <code>/etc/hosts</code> 文件中做了 IP 地址和主机名的映射，那么就可以直接使用主机名，否则就需要写对应的 IP 地址</p>
<p>输入 <code>vi slaves</code>，并在文件中添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop101</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong> 文件中添加的内容结尾不允许有空格，并且文件中不允许有空行。</p>
<h5 id="同步虚拟机文件及配置环境"><a href="#同步虚拟机文件及配置环境" class="headerlink" title="同步虚拟机文件及配置环境"></a>同步虚拟机文件及配置环境</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo rsync -av /opt/module/hadoop-2.6.0-cdh5.15.0 hadoop102:/opt/module/</span><br><span class="line">sudo rsync -av /opt/module/hadoop-2.6.0-cdh5.15.0 hadoop103:/opt/module/</span><br><span class="line">sudo rsync -av /etc/profile hadoop102:/etc/profile</span><br><span class="line">sudo rsync -av /etc/profile hadoop103:/etc/profile</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong> 在完成了同步操作后，还需要在各台虚拟机上执行 <code>source /etc/profile</code> 使配置生效。</p>
<h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><h3 id="格式化-NameNode"><a href="#格式化-NameNode" class="headerlink" title="格式化 NameNode"></a>格式化 NameNode</h3><p>如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong> <strong>NameNode 只在第一次启动时需要格式化</strong>，往后非必要的情况下都不需要格式化，因为频繁的格式化会导致出现一些意想不到的问题，比如找不到 DataNode 等。</p>
<h3 id="启动HDFS和YARN"><a href="#启动HDFS和YARN" class="headerlink" title="启动HDFS和YARN"></a>启动HDFS和YARN</h3><ul>
<li>先在 <code>hadoop101</code> 上启动 HFDS，使用脚本命令 <code>start-dfs.sh</code></li>
<li>再在 <code>hadoop102</code>（包含 ResourceManager） 上启动 YARN，使用脚本命令 <code>start-yarn.sh</code></li>
</ul>
<p><strong>注意</strong> 若 NameNode 和 ResourceMange 不是同一台机器，则不能在 NameNode 上启动 YARN，应该在 ResouceManager 所在的机器上启动 YARN。</p>
<p>接下来使用 jps 命令分别查看一下三台虚拟机的运行情况</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/hadoop101-jps.jpg" alt="hadoop101-jps"></p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/hadoop102-jps.jpg" alt="hadoop102-jps"></p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/hadoop103-jps.jpg" alt="hadoop103-jps"></p>
<p>结果与之前的集群部署规划相符合。至此，Hadoop 分布式集群基本部署完成，再来看一下控制台的情况，以下操作在自己本机（Windows / Mac OS）的浏览器中操作</p>
<p>输入 <code>haoop101:50070</code> 查看 HDFS 的运行情况：</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%9F%A5%E7%9C%8BHDFS%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8A%B6%E5%86%B5.jpg" alt="查看HDFS的运行状况"></p>
<p>再点击头部的 <code>Datanodes</code> 查看 DataNode 的情况</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%9F%A5%E7%9C%8BDataNode%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8A%B6%E5%86%B5.jpg" alt="查看DataNode的运行状况"></p>
<p>可以发现 3 台 DataNode 都已经运行起来了。接着输入 <code>hadoop102:8088</code> 查看 YARN 的运行情况：</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%9F%A5%E7%9C%8BYARN%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8A%B6%E6%80%81.jpg" alt="查看YARN的运行状态"></p>
<p>点击 <code>Active Nodes</code> 下的数字 3，还可以看到红框内存活着的三台 DataNode 的信息</p>
<h3 id="启动历史服务器"><a href="#启动历史服务器" class="headerlink" title="启动历史服务器"></a>启动历史服务器</h3><p>因为之前已经在 <a href="#YARN-%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE">YARN文件配置</a> 和 <a href="#MapReduce-%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE">MapReduce文件配置</a> 这一步中配置过历史服务器了，所以这里可以直接在 <code>hadoop101</code> 上使用命令启动历史服务器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<p>使用 jps 查看启动情况</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%90%AF%E5%8A%A8%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8.jpg" alt="启动历史服务器"></p>
<p>配置历史服务器的目的是为了方便查看程序的历史运行情况。</p>
<h3 id="运行官方示例"><a href="#运行官方示例" class="headerlink" title="运行官方示例"></a>运行官方示例</h3><p>经过以上的铺垫，下面就可以使用官方示例查看一下实际的运行效果</p>
<p>在主机 <code>hadoop101</code> 的 <code>/opt/module/hadoop-2.6.0-cdh5.15.0</code> 目录下，执行如下命令，回车便可看到运行结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">[inno@hadoop101 hadoop-2.6.0-cdh5.15.0]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.15.0.jar pi 3 4</span><br><span class="line">Number of Maps  = 3</span><br><span class="line">Samples per Map = 4</span><br><span class="line">20/07/06 20:57:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Wrote input for Map #0</span><br><span class="line">Wrote input for Map #1</span><br><span class="line">Wrote input for Map #2</span><br><span class="line">Starting Job</span><br><span class="line">20/07/06 20:57:07 INFO client.RMProxy: Connecting to ResourceManager at hadoop102/192.168.15.134:8032</span><br><span class="line">20/07/06 20:57:07 INFO input.FileInputFormat: Total input paths to process : 3</span><br><span class="line">20/07/06 20:57:07 INFO mapreduce.JobSubmitter: number of splits:3</span><br><span class="line">20/07/06 20:57:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1594037734992_0001</span><br><span class="line">20/07/06 20:57:08 INFO impl.YarnClientImpl: Submitted application application_1594037734992_0001</span><br><span class="line">20/07/06 20:57:08 INFO mapreduce.Job: The url to track the job: http://hadoop102:8088/proxy/application_1594037734992_0001/</span><br><span class="line">20/07/06 20:57:08 INFO mapreduce.Job: Running job: job_1594037734992_0001</span><br><span class="line">20/07/06 20:57:17 INFO mapreduce.Job: Job job_1594037734992_0001 running in uber mode : false</span><br><span class="line">20/07/06 20:57:17 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">20/07/06 20:57:24 INFO mapreduce.Job:  map 33% reduce 0%</span><br><span class="line">20/07/06 20:57:27 INFO mapreduce.Job:  map 67% reduce 0%</span><br><span class="line">20/07/06 20:57:28 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">20/07/06 20:57:30 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">20/07/06 20:57:30 INFO mapreduce.Job: Job job_1594037734992_0001 completed successfully</span><br><span class="line">20/07/06 20:57:30 INFO mapreduce.Job: Counters: 49</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=72</span><br><span class="line">		FILE: Number of bytes written=573225</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=792</span><br><span class="line">		HDFS: Number of bytes written=215</span><br><span class="line">		HDFS: Number of read operations=15</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=3</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=3</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=3</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=19491</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=3145</span><br><span class="line">		Total time spent by all map tasks (ms)=19491</span><br><span class="line">		Total time spent by all reduce tasks (ms)=3145</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=19491</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=3145</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=19958784</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=3220480</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=3</span><br><span class="line">		Map output records=6</span><br><span class="line">		Map output bytes=54</span><br><span class="line">		Map output materialized bytes=84</span><br><span class="line">		Input split bytes=438</span><br><span class="line">		Combine input records=0</span><br><span class="line">		Combine output records=0</span><br><span class="line">		Reduce input groups=2</span><br><span class="line">		Reduce shuffle bytes=84</span><br><span class="line">		Reduce input records=6</span><br><span class="line">		Reduce output records=0</span><br><span class="line">		Spilled Records=12</span><br><span class="line">		Shuffled Maps =3</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=3</span><br><span class="line">		GC time elapsed (ms)=398</span><br><span class="line">		CPU time spent (ms)=4400</span><br><span class="line">		Physical memory (bytes) snapshot=1034440704</span><br><span class="line">		Virtual memory (bytes) snapshot=11132063744</span><br><span class="line">		Total committed heap usage (bytes)=859832320</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=354</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=97</span><br><span class="line">Job Finished in 23.456 seconds</span><br><span class="line">Estimated value of Pi is 3.66666666666666666667</span><br></pre></td></tr></table></figure>

<p>看一下控制台的情况，浏览器输入 <code>hadoop102:8088</code>，可以看到官方示例已经运行完成。因为上一步已经配置过了历史服务器，正好可以看一下官方示例的运行过程中，产生的日志，查看步骤如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%9F%A5%E7%9C%8B%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8A%B6%E6%80%811.jpg" alt="查看官方示例的运行状态1"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%9F%A5%E7%9C%8B%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8A%B6%E6%80%812.jpg" alt="查看官方示例的运行状态2"><br><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%9F%A5%E7%9C%8B%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B%E7%9A%84%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97.jpg" alt="查看官方示例的运行日志"></p>
<p>如果我们的 MapReduce 程序出现了错误，那么查看 Log 文件可以很好的帮助我们解决问题。</p>
<h3 id="停止集群"><a href="#停止集群" class="headerlink" title="停止集群"></a>停止集群</h3><p>当不需要运行集群的时候，要注意千万不能直接关闭虚拟机，一定要先停止集群的运行，再关闭虚拟机，否则会造成很多想不到的结果。</p>
<ul>
<li><strong>停止历史服务器</strong>：<code>mr-jobhistory-daemon.sh stop historyserver</code></li>
<li><strong>停止YARN</strong>：<code>stop-yarn.sh</code></li>
<li><strong>停止HDFS</strong>：<code>top-dfs.sh</code></li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="集群时间同步"><a href="#集群时间同步" class="headerlink" title="集群时间同步"></a>集群时间同步</h3><p>经过之前的步骤，Hadoop 完全分布式集群已经配置完成了，所以这一步的集群时间同步与 Hadoop 集群的关系不是特别大。所谓的集群时间同步，就是为了让三台服务器的时间取得一致（仅仅只是保持一致，而非保证时间正确），这么做的目的是因为在之后使用集群的过程中，比如使用 HBase 的时候，对主机和从机的时间同步性要求很高。虽然说集群时间同步的配置与否并不影响 Hadoop，但是在之后的使用过程中，还是需要有这么一个步骤，因此这里建议还是顺便配置一下。</p>
<p>时间同步的原理也很简单：找一个机器，作为时间服务器，集群中其它所有的机器与这台机器的时间进行定时的同步，比如，每隔十分钟同步一次时间。</p>
<p>因为时间同步服务依赖于 NTP 服务，所以首先应该查一下系统是否具有 NTP 服务。<strong>注意</strong>，以下操作都需要在 root 用户下执行，因此在执行所有操作前进行切换 ，使用命令 <code>su root</code> ，如果不切换那么就需要每条命令之前加上 <code>sudo</code>。</p>
<h4 id="检查NTP服务是否存在"><a href="#检查NTP服务是否存在" class="headerlink" title="检查NTP服务是否存在"></a>检查NTP服务是否存在</h4><p>首先，检查 NTP 是否安装 <code>rpm -qa|grep ntp</code>，如果没有输出任何信息，则表示系统没有 NTP 服务，那么输入 <code>yum install ntp</code> 安装一下即可。然后再输入 <code>rpm -qa|grep ntp</code> 检查一下是否有输入，如果出现如下信息，则表示 NTP 服务已存在</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%A3%80%E6%9F%A5NTP%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E5%AE%89%E8%A3%85.jpg" alt="检查NTP服务是否安装"></p>
<h4 id="检查NTPD服务是否开启"><a href="#检查NTPD服务是否开启" class="headerlink" title="检查NTPD服务是否开启"></a>检查NTPD服务是否开启</h4><p>输入 <code>service ntpd status</code>，如果出现如下信息则表示服务未开启</p>
<p><img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E6%A3%80%E6%9F%A5NTP%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E5%BC%80%E5%90%AF.jpg" alt="检查NTPD服务是否开启"></p>
<p>如果显示正在运行，那么需要先将其关闭，否则端口会被占用。关闭的步骤也很简单，依次输入如下两条命令</p>
<ul>
<li>输入 <code>service ntpd stop</code> 即关闭 NTP 服务</li>
<li>然后 <code>chkconfig ntpd off</code></li>
</ul>
<p><strong>注意</strong> 以上  <a href="#%E6%A3%80%E6%9F%A5NTP%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8">检查NTP服务是否存在</a> 和 <a href="#%E6%A3%80%E6%9F%A5NTP%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E5%BC%80%E5%90%AF">检查NTP服务是否开启</a> 需要分别在三台虚拟机上执行，往下开始正式的配置。</p>
<h4 id="修改NTP配置文件"><a href="#修改NTP配置文件" class="headerlink" title="修改NTP配置文件"></a>修改NTP配置文件</h4><p>我们选择 <code>hadoop101</code> 作为时间服务器，因此这一步的修改配置文件将在 <code>hadoop101</code> 的终端上执行，输入 <code>vi /etc/ntp.conf</code></p>
<ol>
<li>找到 <code>#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</code> 这一行，并取消它的注释</li>
<li>找到如下几行并都为其添加注释<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure></li>
<li>在文件末尾添加如下两行（目的是当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>
<img src="https://cdn.jsdelivr.net/gh/innofang/jotter/source/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9NTP%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.jpg" alt="修改NTP配置文件"></li>
</ol>
<h4 id="修改-etc-sysconfig-ntpd文件"><a href="#修改-etc-sysconfig-ntpd文件" class="headerlink" title="修改/etc/sysconfig/ntpd文件"></a>修改<code>/etc/sysconfig/ntpd</code>文件</h4><p>这一步是为了让硬件时间与系统时间一起同步，输入 <code>vi /etc/sysconfig/ntpd</code>，文件末尾添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure>

<h4 id="重启NTP服务并设置开机自启"><a href="#重启NTP服务并设置开机自启" class="headerlink" title="重启NTP服务并设置开机自启"></a>重启NTP服务并设置开机自启</h4><p>先检查一下 NTPD 服务的状态，输入 <code>service ntpd status</code>，一般来说是停止的，因为没停止是不能修改的，但是操作前进行检查还是有必要的。</p>
<ul>
<li>开启 NTPD 服务 <code>service ntpd start</code></li>
<li>设置 NTPD 服务开机自启 <code>chkconfig ntpd on</code></li>
</ul>
<p>至此，<code>hadoop101</code> 作为时间服务器已经配置完成。</p>
<h4 id="配置其它机器与时间服务器进行同步"><a href="#配置其它机器与时间服务器进行同步" class="headerlink" title="配置其它机器与时间服务器进行同步"></a>配置其它机器与时间服务器进行同步</h4><p>在另外两台虚拟机上，配置 10 分钟与时间服务器同步一次，输入 <code>crontab -e</code>，编写定时任务如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop101</span><br></pre></td></tr></table></figure>

<p>这定时任务的意思就是：每隔 10 分钟执行一次 <code>/usr/sbin/ntpdate hadoop101</code> 命令。</p>
<p>至此就完成了集群时间同步的配置。如果想要测试一下，可以修改其他两台机器的时间，比如在另外两台机器的终端中输入 <code>date -s &quot;2020-02-02 20:02:20&quot;</code> 修改一下时间，十分钟后再输入 <code>date</code> 查看时间是否已经恢复。当然为了便于快速看到结果，定时任务的时间间隔可以修改成一分钟（将 10 改为 1 即可）。</p>
<!-- iamge resource -->

</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Inno Fang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://innofang.github.io/2020/06/22/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/">http://innofang.github.io/2020/06/22/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/08/28/Github%E9%A1%B9%E7%9B%AEclone%E9%80%9F%E5%BA%A6%E8%BF%87%E6%85%A2%EF%BC%9F%E8%AF%95%E8%AF%95%E8%BF%99%E6%8B%9B/"><i class="fa fa-chevron-left">  </i><span>Github项目clone速度过慢？试试这招</span></a></div><div class="next-post pull-right"><a href="/2020/04/05/Shell%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><span>Shell编程学习记录</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://ww1.sinaimg.cn/large/0067fiZ7ly1g1i7czlxxhj318g0rsti8.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2022 By Inno Fang</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Programming is an art form.</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>